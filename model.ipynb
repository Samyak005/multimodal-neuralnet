{"cells":[{"metadata":{"id":"q8nGFvhnSAK0","trusted":true},"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence","execution_count":null,"outputs":[]},{"metadata":{"id":"fJ7bnKqlF_gt","trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = torch.load('../input/encodings/instruction_embeddings.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pad_sequence(test, batch_first=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"AZo_8OxIFf3K","trusted":true},"cell_type":"code","source":"instructions = torch.load('../input/10k-encodings/instruction_embeddings_10k.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instructions = instructions[0:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(instructions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instructions = pad_sequence(instructions, batch_first=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZvBXAfOBFzzT","trusted":true},"cell_type":"code","source":"with open(\"../input/10k-encodings/lengths_10k.txt\", \"rb\") as fp:\n    lengths = pickle.load(fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lengths = lengths[0:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/encodings/lengths.txt\", \"rb\") as fp:\n    lengths_test = pickle.load(fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lengths_test = torch.FloatTensor(lengths_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"FAzHyQ0UrqzG","trusted":true},"cell_type":"code","source":"len(instructions[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"AEyv3NouGREj","trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"re6XiZVEGCE9","trusted":true},"cell_type":"code","source":"image_encoding = np.load('../input/encodings/train_image_encoding_2048_1000.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.append([[1,2],[2,3]],[[4,5],[5,6]],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndirectory = '../input/train-image-encoding-2048-2000-5000'\n\nfor i in range(2000,6000,1000):\n    print(i)\n    \n    temp = np.load('../input/train-image-encoding-2048-2000-5000/train_image_encoding_2048_'+str(i)+'.npy')\n    image_encoding = np.append(image_encoding,temp,axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#directory = '../input/train-image-encoding-2048-6000-10000'\n\n#for i in range(6000,11000,1000):\n#    print(i)\n    \n#    temp = np.load('../input/train-image-encoding-2048-6000-10000/train_image_encoding_2048_'+str(i)+'.npy')\n#    image_encoding = np.append(image_encoding,temp,axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_encoding.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_test = torch.from_numpy(image_encoding[:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = pd.DataFrame(instructions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = pd.DataFrame(lengths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = pd.DataFrame(image_encoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [data2,data1,data3]\ndata = pd.concat(frames,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head","execution_count":null,"outputs":[]},{"metadata":{"id":"eMV-Opjuq5nd","trusted":true},"cell_type":"code","source":"class Data(Dataset):\n    \n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        # load image as ndarray type (Height * Width * Channels)\n        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n        # in this example, i don't use ToTensor() method of torchvision.transforms\n        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n        image = torch.tensor(self.data.iloc[index, 2:].values.astype(np.float32))\n        instructions = self.data.iloc[index, 0]\n        lengths = self.data.iloc[index,1]\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return instructions, lengths, image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = Data(data, transform=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instruction, length, image = train_dataset.__getitem__(0)","execution_count":null,"outputs":[]},{"metadata":{"id":"PfEP0Wenr6dY","trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"IYp-n0wRtLkZ","trusted":true},"cell_type":"code","source":"train_iter = iter(train_loader)\nprint(type(train_iter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instructions, lengths, images = train_iter.next()\n\nprint('images shape on batch size = {}'.format(images.size()))\nprint('instructions shape on batch size = {}'.format(instructions.size()))\nprint('lengths shape on batch size = {}'.format(lengths.size()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(lengths)","execution_count":null,"outputs":[]},{"metadata":{"id":"uAFpG9OB0ARF","trusted":true},"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence","execution_count":null,"outputs":[]},{"metadata":{"id":"Wyfp09DBV00w","trusted":true},"cell_type":"code","source":"  \nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch import nn\nimport torch\nimport numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5ORErp2m3TaP","trusted":true},"cell_type":"code","source":"def l2norm(X):\n    \"\"\"L2-normalize columns of X\n    \"\"\"\n    norm = torch.pow(X, 2).sum(dim=1, keepdim=True).sqrt()\n    X = torch.div(X, norm)\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(recipe,images):\n    accuracy = 0\n    r = torch.matmul(recipe, torch.transpose(images,0,1))\n    top_values, top_indices = torch.topk(r,10)\n    for i in range(len(recipe)):\n        for boolean in top_indices[i] == i:\n            if boolean:\n                accuracy+=1\n                break\n    accuracy = (accuracy/len(recipe))*100\n    return accuracy\n                 ","execution_count":null,"outputs":[]},{"metadata":{"id":"-aseZBFK3Yd6","trusted":true},"cell_type":"code","source":"class TextEncoder(nn.Module):\n\n    def __init__(self, embed_dim, hidden_size, output_size, dropout, num_layers, use_abs=False):\n        super(TextEncoder, self).__init__()\n        self.use_abs = use_abs\n        self.hidden_size = hidden_size\n\n        \n        self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, batch_first=True)\n\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n\n        self.init_weights()\n\n    def init_weights(self):\n        # Linear\n        r = np.sqrt(6.) / np.sqrt(self.fc.in_features +\n                                  self.fc.out_features)\n        self.fc.weight.data.uniform_(-r, r)\n        self.fc.bias.data.fill_(0)\n\n        \n\n    def forward(self, x, lengths):\n        \"\"\"Handles variable size captions\n        \"\"\"\n       \n        lengths = torch.FloatTensor(lengths)\n        # pytorch 1.1\n        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        \n        out, _ = self.rnn(packed)\n        \n        padded, _ = pad_packed_sequence(out, batch_first=True)\n\n        # Use final LSTM Output\n        length_tensor = lengths.reshape(-1, 1, 1)  # shape: bs,1,1\n        # Get last index (before padding) of each sentence\n        \n        length_tensor = length_tensor.expand(x.size(0), 1, self.hidden_size) - 1  # bs, 1, hidden_size\n        length_tensor = torch.tensor(length_tensor,dtype=torch.int64)\n        \n        out = torch.gather(padded, 1, length_tensor).squeeze(1)  # bs * hidden_size\n\n        # normalization in the joint embedding space\n        output = l2norm(out)\n        output = self.fc(self.dropout(output))\n\n        # take absolute value, used by order embeddings\n        if self.use_abs:\n            output = torch.abs(out)\n\n        return output","execution_count":null,"outputs":[]},{"metadata":{"id":"GHS26eAMC-j6","trusted":true},"cell_type":"code","source":"from torch.nn.utils.clip_grad import clip_grad_norm_\nfrom torch.optim import Adam, SGD","execution_count":null,"outputs":[]},{"metadata":{"id":"xss4k743pVWS","trusted":true},"cell_type":"code","source":"embed_dim = 4096\nhidden_size = 5000\noutput_size = 2048\ndropout = 0\nnum_layers = 1\nmax_epochs = 15","execution_count":null,"outputs":[]},{"metadata":{"id":"SRkZlOZMrMVa","trusted":true},"cell_type":"code","source":"lr = 0.1","execution_count":null,"outputs":[]},{"metadata":{"id":"7e71XdyqfMTE","trusted":true},"cell_type":"code","source":"y = torch.ones([128], dtype=torch.int32)\n# y is a tensor of 1 and -1, indicating whether it's a positive pair or negative pair","execution_count":null,"outputs":[]},{"metadata":{"id":"Jxy1Ek7nMGjg","outputId":"923bd2df-3919-40f3-d019-0dbbce0c67b3","trusted":true},"cell_type":"code","source":"model = TextEncoder(embed_dim = 4096, hidden_size = 5000, output_size =2048, dropout = 0, num_layers = 1)\nmodel.load_state_dict(torch.load('../input/trained-model/model.pt'))\nloss_function = nn.CosineEmbeddingLoss()\nparams = list(model.parameters())\noptimizer = Adam(params, lr=lr, weight_decay=5e-4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = open('./model.pt', 'w')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 1.0\nfor epoch in range(max_epochs):\n    print(\"Epoch: \",epoch+1,\"/\",max_epochs,end= \" \")\n    train_iter = iter(train_loader)\n    for instructions, lengths, images in train_iter:\n        model.zero_grad()\n        output = model(instructions,lengths.float())\n        loss = loss_function(output,images, torch.ones([len(lengths)], dtype=torch.int32))\n        loss.backward()\n        optimizer.step()\n        print(\"-\",end = \"\")\n    with torch.no_grad():\n        recipe = model(test, lengths_test)\n    max_accuracy = accuracy(recipe,image_test.type(torch.FloatTensor))\n    print(\"> Loss: \",loss,\" Accuracy: \",max_accuracy)\n    if (max_accuracy>temp):\n        torch.save(model.state_dict(), './model.pt')\n        temp = max_accuracy\n    \n     ","execution_count":null,"outputs":[]},{"metadata":{"id":"4T70MdJYvafe","trusted":true},"cell_type":"code","source":"with torch.no_grad():\n        recipe = model(test, lengths_test)\naccuracy(recipe,image_test.type(torch.FloatTensor))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}